{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVr0wRNdjXxA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "- Ensemble Learning is a machine learning technique where multiple models (often called \"weak learners\") are trained and combined to solve a particular problem. Instead of relying on a single model, ensemble methods integrate the predictions of several models to produce a stronger, more accurate, and more robust final prediction.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of models working together can often outperform a single model, especially when the individual models have diverse errors. By aggregating their outputs, ensemble methods reduce variance, bias, or both, and improve generalization.\n",
        "\n",
        "There are three main types of ensemble methods:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): Trains multiple models on different subsets of the training data and averages their predictions (e.g., Random Forest).\n",
        "\n",
        "Boosting: Trains models sequentially, where each new model focuses on correcting the errors of the previous one (e.g., AdaBoost, XGBoost).\n",
        "\n",
        "Stacking: Combines multiple models (base learners) and uses another model (meta-learner) to make the final prediction.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "- Bagging and Boosting are two popular ensemble learning techniques, but they differ in how they build and combine models.\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Approach: Trains multiple models independently on random subsets of the training data (with replacement).\n",
        "\n",
        "Focus: Reduces variance and helps prevent overfitting.\n",
        "\n",
        "Combination: Aggregates predictions by averaging (for regression) or majority voting (for classification).\n",
        "\n",
        "Example Algorithms: Random Forest, Bagged Decision Trees.\n",
        "\n",
        "2. Boosting:\n",
        "\n",
        "Approach: Trains models sequentially, where each new model corrects the errors of the previous one.\n",
        "\n",
        "Focus: Reduces both bias and variance, making the model stronger and more accurate.\n",
        "\n",
        "Combination: Aggregates predictions by weighted majority voting (classification) or weighted sum (regression).\n",
        "\n",
        "Example Algorithms: AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methodslike Random Forest?\n",
        "- Bootstrap sampling is a statistical technique where multiple new datasets are created by randomly sampling, with replacement, from the original dataset. Each bootstrap sample has the same size as the original dataset, but since sampling is done with replacement, some data points may appear multiple times while others may not appear at all.\n",
        "\n",
        "Role in Bagging (e.g., Random Forest):\n",
        "\n",
        "Diversity of Models: In Bagging methods, each model (e.g., decision tree in Random Forest) is trained on a different bootstrap sample. This introduces variability among the models, making them less correlated.\n",
        "\n",
        "Variance Reduction: By training models on different bootstrap samples and then aggregating their predictions, Bagging reduces the overall variance and improves stability.\n",
        "\n",
        "Better Generalization: Bootstrap sampling ensures that models do not overfit to the same dataset. The combination of diverse models leads to better generalization on unseen data.\n",
        "\n",
        "Example in Random Forest:\n",
        "\n",
        "Each decision tree is trained on a bootstrap sample of the data.\n",
        "\n",
        "Predictions are made by aggregating results from all trees (majority vote for classification, average for regression).\n",
        "\n",
        "This use of bootstrap sampling is what gives Random Forest its robustness and accuracy.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "- In Bagging methods (like Random Forest), each model is trained on a bootstrap sample of the dataset. Since sampling is done with replacement, some data points are left out in each bootstrap sample. These unused data points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "1. Out-of-Bag (OOB) Samples:\n",
        "\n",
        "Roughly one-third of the data is not included in a given bootstrap sample.\n",
        "\n",
        "These samples act as a kind of test set for the model that was trained without them.\n",
        "\n",
        "2. OOB Score:\n",
        "\n",
        "After a model is trained on its bootstrap sample, its performance can be evaluated using the OOB samples.\n",
        "\n",
        "For Random Forest, predictions are made on each instance using only the trees that did not include that instance in their bootstrap sample.\n",
        "\n",
        "The OOB score is then calculated as the average prediction accuracy (for classification) or error (for regression) across all OOB samples.\n",
        "\n",
        "3. Benefits of OOB Score:\n",
        "\n",
        "Built-in validation: Provides an unbiased estimate of model performance without needing a separate validation dataset.\n",
        "\n",
        "Efficient use of data: Maximizes training data usage since the same dataset provides both training and validation samples.\n",
        "Model tuning: Helps in selecting parameters (like the number of trees) without cross-validation.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "- Feature importance measures how much each input variable contributes to predicting the target. Both Decision Trees and Random Forests can provide feature importance, but they differ in how it is calculated and interpreted.\n",
        "\n",
        "1. In a Single Decision Tree:\n",
        "\n",
        "Importance is based on how much a feature reduces impurity (e.g., Gini Index, Entropy, or Variance) across all the splits where the feature is used.\n",
        "\n",
        "The more a feature decreases impurity, the higher its importance score.\n",
        "\n",
        "Limitation:\n",
        "\n",
        "Can be biased toward features with many levels or continuous variables.\n",
        "\n",
        "Importance is unstable because small changes in data can alter the tree structure significantly.\n",
        "\n",
        "2. In a Random Forest:\n",
        "\n",
        "Since Random Forest is an ensemble of many trees, feature importance is averaged across all trees.\n",
        "\n",
        "Two main methods are used:\n",
        "\n",
        "Mean Decrease in Impurity (MDI): Average reduction in impurity across all trees.\n",
        "\n",
        "Mean Decrease in Accuracy (MDA): Measures the drop in model accuracy when a feature’s values are randomly permuted.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "More stable and reliable than a single tree.\n",
        "\n",
        "Less biased toward high-cardinality features.\n",
        "\n",
        "Captures the overall contribution of features across diverse trees."
      ],
      "metadata": {
        "id": "JONnh6pFjv8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "# Load the Breast Cancer dataset using\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "# Train a Random Forest Classifier\n",
        "# Print the top 5 most important features based on feature importance scor\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "# Sort features by importance\n",
        "top_features = feature_importances.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Print the top 5 important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kHQFuN_mO9b",
        "outputId": "eb13fa64-1521-483f-b1a6-e6dd2402e453"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:● Train a Bagging Classifier using Decision Trees on the Iris dataset● Evaluate its accuracy and compare with a single Decision\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=0)\n",
        "dt_model.fit(X_train, y_train)\n",
        "acc_dt = accuracy_score(y_test, dt_model.predict(X_test))\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=0),\n",
        "    n_estimators=100,\n",
        "    random_state=0\n",
        ")\n",
        "bag_model.fit(X_train, y_train)\n",
        "acc_bag = accuracy_score(y_test, bag_model.predict(X_test))\n",
        "\n",
        "# Results\n",
        "print(f\"Single Decision Tree Accuracy: {acc_dt:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy:   {acc_bag:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fszQSbqUmotY",
        "outputId": "10be96a8-cd75-430c-f860-f06f31a9ed9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 0.9778\n",
            "Bagging Classifier Accuracy:   0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:● Train a Random Forest Classifier● Tune hyperparameters max_depth and n_estimators using GridSearchCV● Print the best parameters and final accuracy\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Test Accuracy:\", final_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJq9dnjEnQb6",
        "outputId": "75f95d1c-cb27-4e66-e4ae-bfc67c34dcb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
            "Final Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:● Train a Bagging Regressor and a Random Forest Regressor on the CaliforniaHousing dataset● Compare their Mean Squared Errors (MSE)\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor (with Decision Trees as base estimator)\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Errors\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bag)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luzKRwA4nm7F",
        "outputId": "e6f9c20d-a8ef-458e-e90d-7132f95d2812"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2568358813508342\n",
            "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "- Step 1: Choosing Between Bagging and Boosting\n",
        "\n",
        "Since predicting loan default is a high-risk classification problem with potentially imbalanced data, accuracy and robustness are critical.\n",
        "\n",
        "Boosting (e.g., Gradient Boosting, XGBoost, LightGBM) is generally more effective in such tasks because:\n",
        "\n",
        "It reduces both bias and variance.\n",
        "\n",
        "Sequentially focuses on hard-to-classify customers, improving predictive power.\n",
        "\n",
        "Bagging (e.g., Random Forest) can still be a baseline for stability and variance reduction, but Boosting is preferred for stronger predictive accuracy.\n",
        "\n",
        "Step 2: Handling Overfitting\n",
        "\n",
        "Ensemble models like Boosting can overfit if not tuned properly. To prevent this:\n",
        "\n",
        "Use regularization parameters (learning rate, max_depth, subsampling).\n",
        "\n",
        "Apply early stopping based on validation error.\n",
        "\n",
        "Use cross-validation to monitor generalization performance.\n",
        "\n",
        "Ensure proper feature selection/engineering to reduce noise in the model.\n",
        "\n",
        "Step 3: Selecting Base Models\n",
        "\n",
        "Decision Trees are the most common base learners since they capture nonlinear relationships and interactions.\n",
        "\n",
        "For Bagging: Use fully grown or slightly pruned Decision Trees.\n",
        "\n",
        "For Boosting: Use shallow Decision Trees (stumps or depth=3–6) to prevent overfitting while capturing meaningful patterns.\n",
        "\n",
        "Step 4: Evaluating Performance Using Cross-Validation\n",
        "\n",
        "Apply k-fold cross-validation (e.g., 5-fold or 10-fold) to estimate model performance.\n",
        "\n",
        "Use appropriate metrics for imbalanced classification:\n",
        "\n",
        "AUC-ROC, Precision, Recall, F1-score in addition to accuracy.\n",
        "\n",
        "Confusion matrix to understand false positives/negatives (important in loan default prediction).\n",
        "\n",
        "Step 5: Justifying How Ensemble Learning Improves Decision-Making\n",
        "\n",
        "Loan default prediction involves high stakes — false negatives (predicting “no default” when a customer actually defaults) can lead to financial losses.\n",
        "\n",
        "Ensemble learning provides:\n",
        "\n",
        "Higher accuracy and robustness compared to a single model.\n",
        "\n",
        "Reduced variance (Bagging) and reduced bias (Boosting).\n",
        "\n",
        "Better handling of complex relationships in customer demographic and transaction data.\n",
        "\n",
        "Reliable risk assessment for lending decisions, helping minimize defaults while ensuring fair approval for genuine customers.\n"
      ],
      "metadata": {
        "id": "pIPNAhsjoEQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Create synthetic dataset (simulating loan default data)\n",
        "X, y = make_classification(n_samples=2000, n_features=12, n_informative=6,\n",
        "                           n_redundant=2, weights=[0.7, 0.3], random_state=42)\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Bagging model (Random Forest)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "# Step 4: Train Boosting model (AdaBoost)\n",
        "boost = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "boost.fit(X_train, y_train)\n",
        "boost_pred = boost.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, boost_pred))\n",
        "\n",
        "print(\"\\nClassification Report (Boosting):\")\n",
        "print(classification_report(y_test, boost_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro5AMuGZovuY",
        "outputId": "effa074d-a661-45bf-ffe4-cee7bb5c1cb3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.925\n",
            "AdaBoost Accuracy: 0.8866666666666667\n",
            "\n",
            "Classification Report (Boosting):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92       423\n",
            "           1       0.87      0.72      0.79       177\n",
            "\n",
            "    accuracy                           0.89       600\n",
            "   macro avg       0.88      0.84      0.86       600\n",
            "weighted avg       0.89      0.89      0.88       600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}